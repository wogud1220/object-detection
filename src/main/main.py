import os
import json
import pandas as pd

import torch
from PIL import Image, ImageDraw, ImageFont
import cv2
import numpy as np

from tqdm import tqdm
from collections import Counter
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.image as mpimg
import matplotlib.font_manager as fm

from collections import defaultdict
from ultralytics import YOLO
from IPython.display import Image as IPImage, display

from src.datas.PillDataset import PillDataset
#from src.utils import util
from src.utils.make_yaml import make_class_list
from src.utils.make_yaml import get_class_name_en
from src.utils.util import visualize_annotations
from src.utils.util import convert_to_yolo_format
from src.utils.util import check_image_annotations
from src.utils.font import set_font
from src.utils.font import add_font
from src.utils.albumentations_A import train_compose
from src.utils.albumentations_A import val_compose
from src.utils.chageBbox import change_bboxes
from src.utils.korean import set_korean_font

import globals

# ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ (ì••ì¶• í•´ì œí•œ ìœ„ì¹˜)
BASE_DIR = globals.BASE_DIR
JSON_PATH = f"{BASE_DIR}/train_combined.json"

# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
TRAIN_IMG_DIR = f"{BASE_DIR}/train_images"
TRAIN_ANN_DIR = f"{BASE_DIR}/train_annotations"
TEST_IMG_DIR = f"{BASE_DIR}/test_images"

YOLO_DIR = f"{BASE_DIR}/yolo_dataset"

def main():
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    set_korean_font()

    """ main start """
    # GPU ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    """ # ê²½ë¡œ í™•ì¸ """
    check_datapath()

    """ # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¶œë ¥ """
    show_test_images(TEST_IMG_DIR)

    """ # Annotation íŒŒì¼ ìˆ˜ì§‘ ë° í†µí•© """
    train_data, all_json_files = process_annotation(TRAIN_ANN_DIR)
    update_dict = {
        2339: [95, 630, 350, 425],
        2101: [600, 708, 235, 451],
        3679: [88, 864, 250, 230],
        805: [620, 770, 226, 224],
        2789: [590, 295, 210, 215],
        568: [365, 852, 200, 200],
        2374: [115, 853, 227, 226],
        2430: [637, 203, 224, 219],
        2778: [125, 770, 315, 275],
    665: [567, 625, 311, 315],
    972: [653, 889, 217, 217]
    }

    change_bboxes(JSON_PATH, update_dict)

    """ # ë°ì´í„° íƒìƒ‰ """
    images_df, categories_df, annotations_df = search_data(train_data)

    """  TEST   """
    #get_class_name_en(categories_df, images_df)

    ### FONT ###
    #set_font()
    #add_font()

    """ # ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™” """
    process_visualize_annotations(images_df, categories_df, annotations_df)

    # ìœ„ì—ì„œ ë³¸ ì´ë¯¸ì§€ë“¤ í™•ì¸
    check_image_annotations(1023, images_df, annotations_df, categories_df)  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€
    check_image_annotations(599, images_df, annotations_df, categories_df)   # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€

    """ JSON íŒŒì¼ì„ í™•ì¸ """
    check_json(all_json_files)

    """ ë°ì´í„°ì…‹, ë°ì´í„°ë¡œë” ì²˜ë¦¬ """
    train_images_df, val_images_df, train_annotations_df, val_annotations_df = process_data(images_df, categories_df, annotations_df)

    """ YOLO ë°ì´í„°ì…‹ """
    category_id_mapping, num_classes = process_yolo_dataset(categories_df)

    """ # Train, Val ë°ì´í„° ë³€í™˜ """
    train_success, val_success = convert_data(train_images_df, val_images_df, train_annotations_df, val_annotations_df, category_id_mapping)

    """ í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± """
    yaml_path = make_class_list(categories_df, images_df, num_classes, train_success, val_success, YOLO_DIR)

    """ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± """
    model = make_model()

    """ ëª¨ë¸ í•™ìŠµ """
    train_model(model, yaml_path)

    """ ëª¨ë¸ ê²°ê³¼ """
    result_model()

    process_visualize_clean(model, val_images_df, device)

    predict_model(model, val_images_df, val_annotations_df, categories_df, device, category_id_mapping)

    predictions = predict_weight_model(device, TEST_IMG_DIR)

    submission_df = result_submission(predictions, category_id_mapping)

    save_submission(submission_df)

    """ main end """


def check_datapath():
    # ê²½ë¡œ í™•ì¸
    print("ğŸ“‚ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:")
    print(f"BASE_DIR      : {BASE_DIR}")
    print(f"TRAIN_IMG_DIR : {TRAIN_IMG_DIR}")
    print(f"TEST_IMG_DIR  : {TEST_IMG_DIR}")

    # ì‹¤ì œ í´ë” ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    print("ğŸ“‚ ê²½ë¡œ ì„¤ì •:")
    for name, path in [("BASE_DIR", BASE_DIR),
                       ("TRAIN_IMG_DIR", TRAIN_IMG_DIR),
                       ("TRAIN_ANN_DIR", TRAIN_ANN_DIR),
                       ("TEST_IMG_DIR", TEST_IMG_DIR)]:
        exists = "âœ…" if os.path.exists(path) else "âŒ"
        print(f"{exists} {name}: {path}")

def show_test_images(test_img_dir):
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”
    #TEST_IMG_DIR = "/content/data/test_images"

    # íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    test_files = sorted(os.listdir(test_img_dir))

    # ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
    print(f"ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê°œìˆ˜: {len(test_files)}")
    print("ì˜ˆì‹œ íŒŒì¼ëª…:", test_files[:5])

    # ì•ë¶€ë¶„ 9ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
    sample_files = test_files[:9]

    # ì‹œê°í™”
    plt.figure(figsize=(12, 12))
    for i, img_name in enumerate(sample_files):
        img_path = os.path.join(test_img_dir, img_name)
        img = mpimg.imread(img_path)
        plt.subplot(3, 3, i + 1)
        plt.imshow(img)
        plt.title(img_name, fontsize=9)
        plt.axis("off")

    plt.tight_layout()
    plt.show()

# Annotation íŒŒì¼ ìˆ˜ì§‘ ë° í†µí•©
def process_annotation(train_ann_dir):
    # ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°
    all_json_files = []
    for root, dirs, files in os.walk(train_ann_dir):
        for file in files:
            if file.endswith('.json'):
                all_json_files.append(os.path.join(root, file))

    print(f"âœ… ì´ JSON íŒŒì¼ ê°œìˆ˜: {len(all_json_files)}")
    print(f"ì˜ˆì‹œ íŒŒì¼:\n{all_json_files[0]}")

    # file_nameì„ í‚¤ë¡œ í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
    images_dict = {}  # {file_name: image_info}
    annotations_by_image = defaultdict(list)  # {file_name: [annotations]}
    categories_dict = {}  # {category_id: category_name}

    print("\nğŸ“Š JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    for idx, json_path in enumerate(all_json_files):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
            if 'images' in data and len(data['images']) > 0:
                img = data['images'][0]
                file_name = img['file_name']
                #dl_name = img['dl_name']
                #dl_name_en = img['dl_name_en']

                # ì´ë¯¸ì§€ ì •ë³´ëŠ” í•œ ë²ˆë§Œ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
                if file_name not in images_dict:
                    images_dict[file_name] = {
                        'file_name': file_name,
                        'width': img.get('width'),
                        'height': img.get('height'),
                        #'dl_name': img.get('dl_name'),
                        #'dl_name_en': img.get('dl_name_en'),
                    }

            # Annotation ìˆ˜ì§‘ (ê°™ì€ file_nameë¼ë¦¬ ë¬¶ìŒ)
            if 'annotations' in data:
                for ann in data['annotations']:
                    annotations_by_image[file_name].append({
                        'category_id': ann['category_id'],
                        'bbox': ann['bbox'],
                        'area': ann.get('area', ann['bbox'][2] * ann['bbox'][3]),
                        'iscrowd': ann.get('iscrowd', 0)
                    })

            # ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
            if 'categories' in data:
                for cat in data['categories']:
                    categories_dict[cat['id']] = cat['name']

            # ì§„í–‰ìƒí™© ì¶œë ¥ (500ê°œë§ˆë‹¤)
            if (idx + 1) % 500 == 0:
                print(f"  ì²˜ë¦¬ ì¤‘... {idx + 1}/{len(all_json_files)}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ({os.path.basename(json_path)}): {e}")
            continue

    # COCO í˜•ì‹ìœ¼ë¡œ ìµœì¢… ì •ë¦¬
    combined_data = {
        'images': [],
        'annotations': [],
        'categories': []
    }

    image_id = 0
    annotation_id = 0

    print("\nğŸ”— ì´ë¯¸ì§€ì™€ Annotation ì—°ê²° ì¤‘...")
    for file_name, img_info in images_dict.items():
        # ì´ë¯¸ì§€ ì¶”ê°€
        img_info['id'] = image_id
        combined_data['images'].append(img_info)

        # í•´ë‹¹ ì´ë¯¸ì§€ì˜ ëª¨ë“  annotation ì¶”ê°€
        for ann in annotations_by_image[file_name]:
            combined_data['annotations'].append({
                'id': annotation_id,
                'image_id': image_id,
                'category_id': ann['category_id'],
                'bbox': ann['bbox'],
                'area': ann['area'],
                'iscrowd': ann['iscrowd']
            })
            annotation_id += 1

        image_id += 1

    # ì¹´í…Œê³ ë¦¬ ì •ë¦¬
    combined_data['categories'] = [
        {'id': cat_id, 'name': cat_name}
        for cat_id, cat_name in sorted(categories_dict.items())
    ]

    print(f"\nâœ… í†µí•© ì™„ë£Œ!")
    print(f"  - ì´ ì´ë¯¸ì§€: {len(combined_data['images'])}")
    print(f"  - ì´ Annotation: {len(combined_data['annotations'])}")
    print(f"  - ì´ ì¹´í…Œê³ ë¦¬: {len(combined_data['categories'])}")
    print(f"  - í‰ê·  ì´ë¯¸ì§€ë‹¹ ê°ì²´ ìˆ˜: {len(combined_data['annotations']) / len(combined_data['images']):.2f}ê°œ")

    # í†µí•© ë°ì´í„° ì €ì¥
    train_data = combined_data

    # ë‚˜ì¤‘ì— ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒŒì¼ë¡œ ì €ì¥
    output_path = f"{BASE_DIR}/train_combined.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False)
    print(f"\nğŸ’¾ í†µí•© íŒŒì¼ ì €ì¥: {output_path}")

    return train_data, all_json_files

def search_data(train_data):
    # ë°ì´í„° íƒìƒ‰

    # ì´ë¯¸ì§€ ì •ë³´
    images_df = pd.DataFrame(train_data['images'])
    print(f"ğŸ“· ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {len(images_df)}")
    print(images_df.head())

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    categories_df = pd.DataFrame(train_data['categories'])
    print(f"\nğŸ·ï¸ ì´ ì¹´í…Œê³ ë¦¬(ì•Œì•½ ì¢…ë¥˜): {len(categories_df)}")
    print(categories_df)

    # Annotation ì •ë³´
    annotations_df = pd.DataFrame(train_data['annotations'])
    print(f"\nğŸ“¦ ì´ Annotation ê°œìˆ˜: {len(annotations_df)}")
    print(annotations_df.head())

    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
    category_counts = Counter(annotations_df['category_id'])
    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê°ì²´ ê°œìˆ˜:")
    for cat_id, count in sorted(category_counts.items()):
        cat_name = categories_df[categories_df['id'] == cat_id]['name'].values[0]
        print(f"  Class {cat_id} ({cat_name}): {count}ê°œ")

    # ì´ë¯¸ì§€ë‹¹ ê°ì²´ ìˆ˜ ë¶„í¬
    img_obj_counts = annotations_df.groupby('image_id').size()
    print(f"\nğŸ“ˆ ì´ë¯¸ì§€ë‹¹ ê°ì²´ ìˆ˜ í†µê³„:")
    print(f"  - í‰ê· : {img_obj_counts.mean():.2f}ê°œ")
    print(f"  - ìµœì†Œ: {img_obj_counts.min()}ê°œ")
    print(f"  - ìµœëŒ€: {img_obj_counts.max()}ê°œ")

    return images_df, categories_df, annotations_df

def process_visualize_annotations(images_df, categories_df, annotations_df):
    valid_image_ids = annotations_df['image_id'].unique()
    print(f"ğŸ“Š Annotationì´ ìˆëŠ” ì´ë¯¸ì§€: {len(valid_image_ids)}ê°œ")
    print(f"ğŸ“Š ì „ì²´ ì´ë¯¸ì§€: {len(images_df)}ê°œ")

    if len(valid_image_ids) < len(images_df):
        print(f"âš ï¸ Annotationì´ ì—†ëŠ” ì´ë¯¸ì§€: {len(images_df) - len(valid_image_ids)}ê°œ")

    # ê°ì²´ ìˆ˜ë³„ ë¶„í¬ ë‹¤ì‹œ í™•ì¸
    img_obj_counts_df = annotations_df.groupby('image_id').size().reset_index(name='count')
    print(f"\nğŸ“Š ê°ì²´ ìˆ˜ë³„ ì´ë¯¸ì§€ ë¶„í¬:")
    print(img_obj_counts_df['count'].value_counts().sort_index())

    # ì—¬ëŸ¬ ê°ì²´ê°€ ìˆëŠ” ì´ë¯¸ì§€ ì°¾ê¸°
    multi_obj_images = img_obj_counts_df[img_obj_counts_df['count'] >= 2]
    print(f"\nâœ… 2ê°œ ì´ìƒ ê°ì²´: {len(multi_obj_images)}ê°œ")

    # ì‹œê°í™”
    print("\nğŸ¨ ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™” (í¬ê¸° ì¡°ì •):")

    if len(multi_obj_images) > 0:
        # ì—¬ëŸ¬ ê°ì²´ê°€ ìˆëŠ” ì´ë¯¸ì§€ ìš°ì„ 
        print("ì—¬ëŸ¬ ê°ì²´ê°€ ìˆëŠ” ì´ë¯¸ì§€:")
        sample_ids = multi_obj_images['image_id'].sample(min(3, len(multi_obj_images))).values
    else:
        # ì—†ìœ¼ë©´ ëœë¤
        print("ëœë¤ ìƒ˜í”Œ:")
        sample_ids = img_obj_counts_df['image_id'].sample(min(3, len(img_obj_counts_df))).values

    for img_id in sample_ids:
        visualize_annotations(TRAIN_IMG_DIR,
                        images_df,
                        annotations_df,
                        categories_df,
                        img_id,
                        figsize=(8, 8))

def check_json(all_json_files):
    #  ì›ë³¸ JSON íŒŒì¼ì—ì„œ ì§ì ‘ í™•ì¸

    # Image ID 1023ì˜ íŒŒì¼ëª…ìœ¼ë¡œ ì›ë³¸ JSON ì°¾ê¸°
    target_file = "K-001900-016548-031705-033208_0_2_0_2_75_000_200.png"

    print(f"ğŸ” {target_file}ì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ JSON íŒŒì¼ë“¤:\n")

    json_count = 0
    for json_path in all_json_files:
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if 'images' in data and len(data['images']) > 0:
                if data['images'][0]['file_name'] == target_file:
                    json_count += 1
                    print(f"[{json_count}] {os.path.basename(json_path)}")

                    if 'annotations' in data:
                        for ann in data['annotations']:
                            cat_id = ann['category_id']
                            # categoriesì—ì„œ ì´ë¦„ ì°¾ê¸°
                            cat_name = "Unknown"
                            if 'categories' in data:
                                for cat in data['categories']:
                                    if cat['id'] == cat_id:
                                        cat_name = cat['name']
                                        break
                            bbox = ann['bbox']
                            print(f"    - {cat_name} (ID: {cat_id})")
                            print(f"      BBox: {bbox}")
                    print()
        except:
            continue

    print(f"âœ… ì´ {json_count}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
    print(f"\nğŸ’¡ ê²°ë¡ : ì›ë³¸ ë°ì´í„°ì—ë„ {json_count}ê°œì˜ annotationë§Œ ìˆìŒ")
    print("    â†’ ë³‘í•© ê³¼ì •ì€ ì •ìƒì´ë©°, ë°ì´í„°ì…‹ ìì²´ê°€ ì´ë ‡ê²Œ ì œê³µ")

def process_data(images_df, categories_df, annotations_df):
    # Train/Val Split
    train_ids, val_ids = train_test_split(
        images_df['id'].values,
        test_size=0.2,
        random_state=42
    )

    train_images_df = images_df[images_df['id'].isin(train_ids)].reset_index(drop=True)
    val_images_df = images_df[images_df['id'].isin(val_ids)].reset_index(drop=True)

    train_annotations_df = annotations_df[annotations_df['image_id'].isin(train_ids)]
    val_annotations_df = annotations_df[annotations_df['image_id'].isin(val_ids)]

    train_transform = train_compose()
    val_transform = val_compose()

    # Dataset ìƒì„±
    train_dataset = PillDataset(
        TRAIN_IMG_DIR,
        train_images_df,
        train_annotations_df,
        categories_df,
        transform=train_transform
    )

    val_dataset = PillDataset(
        TRAIN_IMG_DIR,
        val_images_df,
        val_annotations_df,
        categories_df,
        transform=val_transform
    )

    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=4,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2
    )

    print("âœ… ë°ì´í„° ì¦ê°•ì´ ì ìš©ëœ Dataset/DataLoader ìƒì„± ì™„ë£Œ!")
    print(f"  - Train: {len(train_dataset)}ê°œ")
    print(f"  - Val: {len(val_dataset)}ê°œ")
    print(f"  - Train batches: {len(train_loader)}")
    print(f"  - Val batches: {len(val_loader)}")

    # ìƒ˜í”Œ í™•ì¸
    images, targets = next(iter(train_loader))
    print(f"\nâœ… ìƒ˜í”Œ ë°°ì¹˜:")
    print(f"  - Batch size: {len(images)}")
    print(f"  - ì´ë¯¸ì§€ shape: {images[0].shape}")
    print(f"  - ê°ì²´ ìˆ˜: {len(targets[0]['labels'])}ê°œ")

    return train_images_df, val_images_df, train_annotations_df, val_annotations_df

# Collate í•¨ìˆ˜
def collate_fn(batch):
    return tuple(zip(*batch))

def process_yolo_dataset(categories_df):
    # YOLO ë°ì´í„°ì…‹ í´ë” êµ¬ì¡° ìƒì„±
    #YOLO_DIR = f"{BASE_DIR}/yolo_dataset"
    os.makedirs(f"{YOLO_DIR}/images/train", exist_ok=True)
    os.makedirs(f"{YOLO_DIR}/images/val", exist_ok=True)
    os.makedirs(f"{YOLO_DIR}/labels/train", exist_ok=True)
    os.makedirs(f"{YOLO_DIR}/labels/val", exist_ok=True)

    print("âœ… YOLO í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ!")

    # ì¹´í…Œê³ ë¦¬ IDë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§¤í•‘
    category_id_mapping = {cat_id: idx for idx, cat_id in enumerate(sorted(categories_df['id'].unique()))}
    num_classes = len(category_id_mapping)

    print(f"ğŸ“Š ì´ í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ")
    print(f"ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ì²˜ìŒ 5ê°œ): {dict(list(category_id_mapping.items())[:5])}")

    return category_id_mapping, num_classes

def convert_data(train_images_df, val_images_df, train_annotations_df, val_annotations_df, category_id_mapping):
    # Train ë°ì´í„° ë³€í™˜
    print("ğŸ“ Train ë°ì´í„° ë³€í™˜ ì¤‘...")
    train_success = 0
    for _, img_info in tqdm(train_images_df.iterrows(), total=len(train_images_df)):
        if convert_to_yolo_format(
                img_info,
                train_annotations_df,
                f"{YOLO_DIR}/images/train",
                f"{YOLO_DIR}/labels/train",
                category_id_mapping,
                TRAIN_IMG_DIR
        ):
            train_success += 1

    print(f"âœ… Train ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {train_success}/{len(train_images_df)}ê°œ")

    # Val ë°ì´í„° ë³€í™˜
    print("\nğŸ“ Val ë°ì´í„° ë³€í™˜ ì¤‘...")
    val_success = 0
    for _, img_info in tqdm(val_images_df.iterrows(), total=len(val_images_df)):
        if convert_to_yolo_format(
                img_info,
                val_annotations_df,
                f"{YOLO_DIR}/images/val",
                f"{YOLO_DIR}/labels/val",
                category_id_mapping,
                TRAIN_IMG_DIR
        ):
            val_success += 1

    print(f"âœ… Val ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {val_success}/{len(val_images_df)}ê°œ")

    return train_success, val_success

def make_model():
    model = YOLO('yolov8m.pt')
    return model

def train_model(model, yaml_path):
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    results = model.train(
        data=yaml_path,
        epochs=1,  ##20,  # ìµœëŒ€ 20 ì—í­  ##ì„ì‹œë¡œ ì—í­ì„ 1ë¡œ ì„¤ì •í•¨.
        imgsz=800,  # ì´ë¯¸ì§€ í¬ê¸°
        batch=8,  # ë°°ì¹˜ í¬ê¸°
        patience=10,  # Early stopping patience (10 ì—í­ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)
        save=True,  # ëª¨ë¸ ì €ì¥
        device=0 if torch.cuda.is_available() else 'cpu',  # GPU ìë™ ì„ íƒ
        project=f'{BASE_DIR}/yolo_runs',  # ê²°ê³¼ ì €ì¥ í´ë”
        name='pill_detection',
        exist_ok=True,
        pretrained=True,
        optimizer='Adam',
        lr0=0.001,  # ì´ˆê¸° learning rate
        lrf=0.01,  # ìµœì¢… learning rate
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=3,
        box=7.5,  # box loss gain
        cls=0.5,  # cls loss gain
        dfl=1.5,  # dfl loss gain
        label_smoothing=0.0,
        val=True,  # Validation ìˆ˜í–‰
        plots=True,  # í•™ìŠµ ê·¸ë˜í”„ ìë™ ìƒì„±
        verbose=True
    )

    print("\n í•™ìŠµ ì™„ë£Œ!")
    print(f" ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {BASE_DIR}/yolo_runs/pill_detection")

def result_model():
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = globals.FONT_TYPE  ##'NanumBarunGothic'
    plt.rcParams['axes.unicode_minus'] = False

    # í°íŠ¸ ê²½ë¡œ ì§€ì • (ìœˆë„ìš° ê¸°ë³¸ í°íŠ¸ í´ë”)
    font_path = globals.FONT_PATH

    # FontProperties ê°ì²´ ìƒì„±
    font_prop = fm.FontProperties(fname=font_path, size=15)

    #font_name = fm.FontProperties(fname=font_path).get_name()
    #plt.rc('font', family=font_name)

    # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì„¤ì •
    result_dir = f"{BASE_DIR}/yolo_runs/pill_detection"

    print("ğŸ“ˆ YOLOv8 í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)


    # 1ï¸âƒ£ Loss ê·¸ë˜í”„
    results_img = f"{result_dir}/results.png"
    if os.path.exists(results_img):
        print("\n1. ğŸ”¹ Loss ë³€í™” ê·¸ë˜í”„")
        img = mpimg.imread(results_img)
        plt.figure(figsize=(14, 8))
        plt.imshow(img)
        plt.axis('off')
        plt.title('í•™ìŠµ ê²°ê³¼ (Loss, mAP, Precision, Recall)', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ results.pngë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2ï¸âƒ£ Confusion Matrix
    cm_img = f"{result_dir}/confusion_matrix.png"
    if os.path.exists(cm_img):
        print("\n2. ğŸ”¹ Confusion Matrix")
        img = mpimg.imread(cm_img)
        plt.figure(figsize=(12, 10))
        plt.imshow(img)
        plt.axis('off')
        plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ confusion_matrix.pngë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 3ï¸âƒ£ Box Precision Curve (ì˜¬ë°”ë¥¸ íŒŒì¼ëª…!)
    boxp_img = f"{result_dir}/BoxP_curve.png"
    if os.path.exists(boxp_img):
        print("\n3. ğŸ”¹ Box Precision Curve")
        img = mpimg.imread(boxp_img)
        plt.figure(figsize=(10, 8))
        plt.imshow(img)
        plt.axis('off')
        plt.title('ì •ë°€ë„ ê³¡ì„  (Precision Curve)', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ BoxP_curve.pngë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 4ï¸âƒ£ Box F1 Curve (ì˜¬ë°”ë¥¸ íŒŒì¼ëª…!)
    boxf1_img = f"{result_dir}/BoxF1_curve.png"
    if os.path.exists(boxf1_img):
        print("\n4. ğŸ”¹ Box F1 Score Curve")
        img = mpimg.imread(boxf1_img)
        plt.figure(figsize=(10, 8))
        plt.imshow(img)
        plt.axis('off')
        plt.title('F1 ì ìˆ˜ ê³¡ì„  (F1 Curve)', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ BoxF1_curve.pngë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 5ï¸âƒ£ Box Precision-Recall Curve
    boxpr_img = f"{result_dir}/BoxPR_curve.png"
    if os.path.exists(boxpr_img):
        print("\n5. ğŸ”¹ Precision-Recall Curve")
        img = mpimg.imread(boxpr_img)
        plt.figure(figsize=(10, 8))
        plt.imshow(img)
        plt.axis('off')
        plt.title('ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„  (PR Curve)', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ BoxPR_curve.pngë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 6ï¸âƒ£ Validation ì˜ˆì¸¡ ê²°ê³¼
    val_batch0 = f"{result_dir}/val_batch0_pred.jpg"
    if os.path.exists(val_batch0):
        print("\n6. ğŸ”¹ Validation ì˜ˆì¸¡ ê²°ê³¼ (Batch 0)")
        img = mpimg.imread(val_batch0)
        plt.figure(figsize=(16, 12))
        plt.imshow(img)
        plt.axis('off')
        plt.title('ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼', fontsize=14, pad=10, fontproperties=font_prop)
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ val_batch0_pred.jpgë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 7ï¸âƒ£ ìµœì¢… ì„±ëŠ¥ ì§€í‘œ
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ì§€í‘œ")
    print("=" * 60)

    csv_path = f"{result_dir}/results.csv"
    if os.path.exists(csv_path):
        import pandas as pd
        results_df = pd.read_csv(csv_path)
        results_df.columns = results_df.columns.str.strip()

        # ë§ˆì§€ë§‰ epoch
        last_row = results_df.iloc[-1]

        print(f"\nğŸ† ìµœì¢… Epoch {int(last_row['epoch'])} ê²°ê³¼:")
        print(f"  â€¢ mAP50-95: {last_row['metrics/mAP50-95(B)']:.4f} ")
        print(f"  â€¢ mAP50:    {last_row['metrics/mAP50(B)']:.4f}")
        print(f"  â€¢ Precision: {last_row['metrics/precision(B)']:.4f}")
        print(f"  â€¢ Recall:    {last_row['metrics/recall(B)']:.4f}")

        # Best ê°’
        best_map = results_df['metrics/mAP50-95(B)'].max()
        best_epoch = results_df['metrics/mAP50-95(B)'].idxmax() + 1
        print(f"\nğŸ¥‡ Best mAP50-95: {best_map:.4f} (Epoch {best_epoch})")
    else:
        print("âŒ results.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 8ï¸âƒ£ Best ëª¨ë¸ ê²½ë¡œ
    best_model = f"{result_dir}/weights/best.pt"
    print(f"\nğŸ’¾ Best ëª¨ë¸ ê²½ë¡œ:")
    if os.path.exists(best_model):
        print(f"   âœ… {best_model}")
        size_mb = os.path.getsize(best_model) / (1024 * 1024)
        print(f"   ğŸ“¦ íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB")
    else:
        print(f"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì™„ë£Œ!")
    print("=" * 60)

def visualize_clean(img_path, model, device, conf_threshold=0.35, iou_threshold=0.5):
    """
    ê²¹ì¹¨ ì—†ëŠ” ê¹”ë”í•œ ì‹œê°í™”
    """

    # ì˜ˆì¸¡ (threshold ì¡°ì •)
    results = model.predict(
        img_path,
        conf=conf_threshold,    # ë‚®ì€ confidence ì œì™¸
        iou=iou_threshold,      # ê²¹ì¹˜ëŠ” ë°•ìŠ¤ ì œê±°
        max_det=4,              # ìµœëŒ€ 4ê°œ
        device=device,
        verbose=False
    )
    result = results[0]

    # ì´ë¯¸ì§€ ë¡œë“œ
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # PILë¡œ ë³€í™˜
    img_pil = Image.fromarray(img)
    draw = ImageDraw.Draw(img_pil)

    # í°íŠ¸ ë¡œë“œ
    try:
        font = ImageFont.truetype(globals.FONT_PATH, 16)
    except:
        font = ImageFont.load_default()

    # ë°•ìŠ¤ë³„ë¡œ ìœ„ì¹˜ ì¡°ì •í•˜ì—¬ ê²¹ì¹¨ ë°©ì§€
    boxes_info = []
    for box in result.boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        conf = float(box.conf[0])
        cls = int(box.cls[0])
        class_name = result.names[cls]

        # ì´ë¦„ ì§§ê²Œ
        if len(class_name) > 12:
            class_name = class_name[:12] + '...'

        boxes_info.append({
            'box': (x1, y1, x2, y2),
            'conf': conf,
            'cls': cls,
            'name': class_name
        })

    # confidence ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    boxes_info.sort(key=lambda x: x['conf'], reverse=True)

    # ê·¸ë¦¬ê¸°
    for idx, info in enumerate(boxes_info):
        x1, y1, x2, y2 = info['box']

        # ìƒ‰ìƒ
        np.random.seed(info['cls'])
        color = tuple(np.random.randint(100, 255, 3).tolist())

        # ë°•ìŠ¤
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)

        # ë¼ë²¨ ìœ„ì¹˜ ì¡°ì • (ìœ„ìª½ì— ê³µê°„ ì—†ìœ¼ë©´ ì•„ë˜ë¡œ)
        label = f"{info['name']} {info['conf']:.2f}"
        bbox = draw.textbbox((0, 0), label, font=font)
        text_w = bbox[2] - bbox[0]
        text_h = bbox[3] - bbox[1]

        # ìœ„ìª½ ê³µê°„ í™•ì¸
        if y1 - text_h - 8 < 0:
            # ì•„ë˜ìª½ì— í‘œì‹œ
            text_y = y2 + 2
            bg_y1, bg_y2 = y2, y2 + text_h + 6
        else:
            # ìœ„ìª½ì— í‘œì‹œ
            text_y = y1 - text_h - 4
            bg_y1, bg_y2 = y1 - text_h - 8, y1

        # ë°°ê²½
        draw.rectangle([x1, bg_y1, x1 + text_w + 6, bg_y2], fill=color)

        # í…ìŠ¤íŠ¸
        draw.text((x1 + 3, text_y), label, fill=(255, 255, 255), font=font)

    return np.array(img_pil)

def process_visualize_clean(model, val_images_df, device):
    # ìƒ˜í”Œ ì‹œê°í™”
    sample_images = val_images_df.sample(6)

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for idx, (_, img_info) in enumerate(sample_images.iterrows()):
        img_path = os.path.join(TRAIN_IMG_DIR, img_info['file_name'])

        img_result = visualize_clean(
            img_path,
            model,
            device,
            conf_threshold=0.4,
            iou_threshold=0.5
        )

        axes[idx].imshow(img_result)
        axes[idx].set_title(f"ID: {img_info['id']}", fontsize=11)
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(f"{BASE_DIR}/yolo_clean_predictions.png", dpi=120)
    plt.show()


def predict_model(model, val_images_df, val_annotations_df, categories_df, device, category_id_mapping):
    print("ğŸ”¬ ì •í™•í•œ mAP@[0.75:0.95] ê³„ì‚°")
    print("=" * 60)

    # 1. Validation ë°ì´í„°ë¡œ ì˜ˆì¸¡
    predictions_list = []

    print("\nğŸ“Š Validation ì˜ˆì¸¡ ì¤‘...")
    for _, img_info in val_images_df.iterrows():
        img_id = int(img_info['id'])
        img_path = os.path.join(TRAIN_IMG_DIR, img_info['file_name'])

        # ì˜ˆì¸¡
        results = model.predict(img_path, conf=0.001, device=device, verbose=False)
        result = results[0]

        # COCO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for box in result.boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            yolo_cls = int(box.cls[0])

            # ì›ë³¸ ì¹´í…Œê³ ë¦¬ ID
            category_id = None
            for orig_id, yolo_id in category_id_mapping.items():
                if yolo_id == yolo_cls:
                    category_id = int(orig_id)
                    break

            if category_id is None:
                continue

            # COCO bbox í˜•ì‹: [x, y, width, height]
            predictions_list.append({
                'image_id': img_id,
                'category_id': category_id,
                'bbox': [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],
                'score': conf
            })

    print(f"âœ… ì´ {len(predictions_list)}ê°œ ì˜ˆì¸¡ ì™„ë£Œ")

    # 2. COCO GT ì¤€ë¹„ (ì™„ì „í•œ í˜•ì‹)
    gt_annotations = {
        'info': {
            'description': 'Pill Detection Validation',
            'version': '1.0',
            'year': 2025
        },
        'licenses': [],
        'images': [],
        'annotations': [],
        'categories': []
    }

    # ì´ë¯¸ì§€ ì •ë³´
    for _, img in val_images_df.iterrows():
        gt_annotations['images'].append({
            'id': int(img['id']),
            'file_name': str(img['file_name']),
            'width': int(img['width']),
            'height': int(img['height'])
        })

    # Annotation ì •ë³´
    for _, ann in val_annotations_df.iterrows():
        gt_annotations['annotations'].append({
            'id': int(ann['id']),
            'image_id': int(ann['image_id']),
            'category_id': int(ann['category_id']),
            'bbox': [float(x) for x in ann['bbox']],
            'area': float(ann['area']),
            'iscrowd': int(ann.get('iscrowd', 0))
        })

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    for _, cat in categories_df.iterrows():
        gt_annotations['categories'].append({
            'id': int(cat['id']),
            'name': str(cat['name'])
        })

    print(f"âœ… GT ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

    # 3. JSON ì €ì¥
    gt_path = f"{BASE_DIR}/val_gt_coco.json"
    pred_path = f"{BASE_DIR}/val_pred_coco.json"

    with open(gt_path, 'w') as f:
        json.dump(gt_annotations, f, indent=2)

    with open(pred_path, 'w') as f:
        json.dump(predictions_list, f, indent=2)

    print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"   GT: {gt_path}")
    print(f"   Pred: {pred_path}")

    # 4. COCO í‰ê°€
    print("\nğŸ“Š COCO í‰ê°€ ì‹¤í–‰ ì¤‘...")
    coco_gt = COCO(gt_path)
    coco_dt = coco_gt.loadRes(pred_path)

    # mAP@[0.75:0.95] ê³„ì‚°
    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
    coco_eval.params.iouThrs = np.array([0.75, 0.80, 0.85, 0.90, 0.95])
    coco_eval.evaluate()
    coco_eval.accumulate()

    print("\nğŸ¯ ëŒ€íšŒ í‰ê°€ ì§€í‘œ (mAP@[0.75:0.95]):")
    print("=" * 60)
    coco_eval.summarize()

    # mAP ì¶”ì¶œ
    map_75_95_exact = coco_eval.stats[0]

    print(f"\nğŸ† ìµœì¢… ê²°ê³¼:")
    print(f"  mAP@[0.75:0.95]: {map_75_95_exact:.4f} ")
    print(f"  (IoU 0.75, 0.80, 0.85, 0.90, 0.95ì˜ í‰ê· )")
    print("=" * 60)

def predict_weight_model(device, test_img_dir):
    # Best ëª¨ë¸ ë¡œë“œ
    best_model_path = f"{BASE_DIR}/yolo_runs/pill_detection/weights/best.pt"
    model = YOLO(best_model_path)

    # Test ì´ë¯¸ì§€ ëª©ë¡
    test_img_dir = f"{BASE_DIR}/test_images"
    test_images = sorted(os.listdir(test_img_dir))

    print(f"Test ì´ë¯¸ì§€ ê°œìˆ˜: {len(test_images)}")

    # ì¶”ë¡ 
    predictions = {}
    for img_name in tqdm(test_images):
        img_path = os.path.join(test_img_dir, img_name)
        results = model.predict(img_path, conf=0.51, iou=0.5, max_det=4, device=device, verbose=False)
        predictions[img_name] = results[0]

    return predictions

def result_submission(predictions, category_id_mapping):
    submission_rows = []
    annotation_id = 1

    for img_name, result in predictions.items():
        # image_id: íŒŒì¼ëª…ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
        image_id = int(img_name.replace('.png', '').replace('.jpg', ''))

        # ê° ë°•ìŠ¤ë§ˆë‹¤ í•œ í–‰
        for box in result.boxes:
            yolo_cls = int(box.cls[0])

            # ì›ë³¸ ì¹´í…Œê³ ë¦¬ ID
            category_id = None
            for orig_id, yolo_id in category_id_mapping.items():
                if yolo_id == yolo_cls:
                    category_id = int(orig_id)
                    break

            if category_id is None:
                continue

            score = float(box.conf[0])

            # BBox
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            bbox_x = int(x1)
            bbox_y = int(y1)
            bbox_w = int(x2 - x1)
            bbox_h = int(y2 - y1)

            submission_rows.append({
                'annotation_id': annotation_id,
                'image_id': image_id,
                'category_id': category_id,
                'bbox_x': bbox_x,
                'bbox_y': bbox_y,
                'bbox_w': bbox_w,
                'bbox_h': bbox_h,
                'score': score
            })

            annotation_id += 1

    # DataFrame
    submission_df = pd.DataFrame(submission_rows)

    print(submission_df.head(5))

    return submission_df

def save_submission(submission_df):
    submission_path = f"{BASE_DIR}/submission.csv"
    submission_df.to_csv(submission_path, index=False)

    print(f"ì €ì¥ ì™„ë£Œ: {submission_path}")

    # í—¤ë” í™•ì¸
    with open(submission_path, 'r') as f:
        print(f" í—¤ë”:")
        print(f.readline().strip())
        print(f"ì²« 5ì¤„:")
        f.seek(0)
        for i, line in enumerate(f):
            if i < 6:
                print(line.strip())


if __name__ == "__main__":
    main()